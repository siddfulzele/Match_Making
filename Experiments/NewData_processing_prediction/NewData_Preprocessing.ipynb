{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv('Rawdata_withClusterLabels.csv')\n",
    "data=pd.read_csv('data.csv')\n",
    "data.head(2)\n",
    "data.drop(['user_id','username'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>height</th>\n",
       "      <th>job</th>\n",
       "      <th>location</th>\n",
       "      <th>pets</th>\n",
       "      <th>smokes</th>\n",
       "      <th>language</th>\n",
       "      <th>new_languages</th>\n",
       "      <th>body_profile</th>\n",
       "      <th>education_level</th>\n",
       "      <th>dropped_out</th>\n",
       "      <th>bio</th>\n",
       "      <th>interests</th>\n",
       "      <th>other_interests</th>\n",
       "      <th>location_preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>single</td>\n",
       "      <td>f</td>\n",
       "      <td>gay</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>66.0</td>\n",
       "      <td>medicine / health</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), spanish (poorly), sign lan...</td>\n",
       "      <td>interested</td>\n",
       "      <td>athletic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>bottom line i love life! i work hard and i lov...</td>\n",
       "      <td>sports</td>\n",
       "      <td>instruments</td>\n",
       "      <td>same state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>gay</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>68.0</td>\n",
       "      <td>other</td>\n",
       "      <td>pleasant hill, california</td>\n",
       "      <td>likes dogs</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), tagalog (okay), french (po...</td>\n",
       "      <td>interested</td>\n",
       "      <td>fit</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>i'm a straightforward, genuine, fun loving (i'...</td>\n",
       "      <td>painting</td>\n",
       "      <td>instruments</td>\n",
       "      <td>anywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>seeing someone</td>\n",
       "      <td>f</td>\n",
       "      <td>bisexual</td>\n",
       "      <td>socially</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>69.0</td>\n",
       "      <td>other</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>has dogs and likes cats</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english (fluently), sign language (poorly), fr...</td>\n",
       "      <td>interested</td>\n",
       "      <td>fit</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>mmmmm yummy tacosss. yoga is where it's at. i ...</td>\n",
       "      <td>instruments</td>\n",
       "      <td>dancing</td>\n",
       "      <td>same city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>bisexual</td>\n",
       "      <td>socially</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>68.0</td>\n",
       "      <td>computer / hardware / software</td>\n",
       "      <td>daly city, california</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>no</td>\n",
       "      <td>english</td>\n",
       "      <td>not interested</td>\n",
       "      <td>average</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "      <td>i'm a stealth geek. that special mix of techni...</td>\n",
       "      <td>sketching</td>\n",
       "      <td>acting</td>\n",
       "      <td>same city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>single</td>\n",
       "      <td>f</td>\n",
       "      <td>bisexual</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>68.0</td>\n",
       "      <td>other</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>yes</td>\n",
       "      <td>english</td>\n",
       "      <td>not interested</td>\n",
       "      <td>average</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>with the whisper of the wind i was weaved into...</td>\n",
       "      <td>craft</td>\n",
       "      <td>designing</td>\n",
       "      <td>same city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          status sex orientation    drinks      drugs  height  \\\n",
       "0   27          single   f         gay  socially      never    66.0   \n",
       "1   26          single   m         gay  socially      never    68.0   \n",
       "2   20  seeing someone   f    bisexual  socially  sometimes    69.0   \n",
       "3   27          single   m    bisexual  socially  sometimes    68.0   \n",
       "4   22          single   f    bisexual     often  sometimes    68.0   \n",
       "\n",
       "                              job                   location  \\\n",
       "0               medicine / health        oakland, california   \n",
       "1                           other  pleasant hill, california   \n",
       "2                           other        oakland, california   \n",
       "3  computer / hardware / software      daly city, california   \n",
       "4                           other        oakland, california   \n",
       "\n",
       "                        pets     smokes  \\\n",
       "0  likes dogs and likes cats         no   \n",
       "1                 likes dogs         no   \n",
       "2    has dogs and likes cats  sometimes   \n",
       "3  likes dogs and likes cats         no   \n",
       "4  likes dogs and likes cats        yes   \n",
       "\n",
       "                                            language   new_languages  \\\n",
       "0  english (fluently), spanish (poorly), sign lan...      interested   \n",
       "1  english (fluently), tagalog (okay), french (po...      interested   \n",
       "2  english (fluently), sign language (poorly), fr...      interested   \n",
       "3                                            english  not interested   \n",
       "4                                            english  not interested   \n",
       "\n",
       "  body_profile  education_level dropped_out  \\\n",
       "0     athletic              4.0          no   \n",
       "1          fit              3.0          no   \n",
       "2          fit              2.0          no   \n",
       "3      average              3.0          no   \n",
       "4      average              2.0         yes   \n",
       "\n",
       "                                                 bio    interests  \\\n",
       "0  bottom line i love life! i work hard and i lov...       sports   \n",
       "1  i'm a straightforward, genuine, fun loving (i'...     painting   \n",
       "2  mmmmm yummy tacosss. yoga is where it's at. i ...  instruments   \n",
       "3  i'm a stealth geek. that special mix of techni...    sketching   \n",
       "4  with the whisper of the wind i was weaved into...        craft   \n",
       "\n",
       "  other_interests location_preference  \n",
       "0     instruments          same state  \n",
       "1     instruments            anywhere  \n",
       "2         dancing           same city  \n",
       "3          acting           same city  \n",
       "4       designing           same city  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age']=data['age'].apply(lambda x: 58 if x>58 else x)\n",
    "data['age']=data['age'].apply(lambda x:'18-30' if x<=30 else('31_40' if x>30 and x <=40 else('41_50' if x>40 and x<=50 else '50+')))\n",
    "dummies=pd.get_dummies(data['age'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('age',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['status', 'sex', 'orientation', 'drinks', 'drugs', 'height', 'job',\n",
       "       'location', 'pets', 'smokes', 'language', 'new_languages',\n",
       "       'body_profile', 'education_level', 'dropped_out', 'bio', 'interests',\n",
       "       'other_interests', 'location_preference', '31_40', '41_50', '50+'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['drinks']=data['drinks'].apply(lambda x:'rarely' if x=='very often' else ('often' if x=='desperately' else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['drugs']=data['drugs'].apply(lambda x: 'sometimes' if x=='often' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data[['drinks','drugs','orientation']],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data=data.drop(['drinks','drugs','orientation'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job']=data['job'].apply(lambda x:'science / tech / engineering' if x=='computer / hardware / software'\n",
    "                                       else('other' if (x=='rather not say' or x=='unemployed' or x=='retired' or x=='military' or\n",
    "                                                       x=='transportation' or x=='political / government' or x=='clerical / administrative'\n",
    "                                                       or x=='hospitality / travel'  or x=='construction / craftsmanship' or x=='law / legal services')\n",
    "                                           else x)\n",
    "                                       )\n",
    "data['job']=data['job'].apply(lambda x:'other_job' if x=='other' else x)\n",
    "dummies=pd.get_dummies(data['job'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('job',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['smokes']=data['smokes'].apply(lambda x: 'sometimes' if x=='when drinking' or x=='trying to quit' else x)\n",
    "\n",
    "dummies=pd.get_dummies(data['smokes'],drop_first=True)\n",
    "dummies=dummies.rename(columns={'sometimes':'smokes_sometimes','yes':'smokes_yes'})\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('smokes',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['body_profile']=data['body_profile'].apply(lambda x:'skinny' if (x=='skinny' or x=='thin' or x=='used up')\n",
    "                                                       else('athletic' if(x=='athletic' or x=='jacked') \n",
    "                                                       else('fit' if (x=='average' or x=='fit')\n",
    "                                                       else('above average' if (x=='curvy' or x=='full figured' or x=='a little extra')else 'fat'))))\n",
    "\n",
    "dummies=pd.get_dummies(data['body_profile'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('body_profile',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['status']=data['status'].apply(lambda x:'single' if x=='single' else 'other')\n",
    "\n",
    "dummies=pd.get_dummies(data['status'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('status',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['interests']=data['interests'].apply(lambda x:'music' if x=='music' or x=='singinig' or x=='instruments' or x=='dance'\n",
    "                                             else('artist' if x=='calligraphy' or x=='diy' or x=='painting' or x=='sketching' or x=='designing' or x=='craft' \n",
    "                                             else('game_video' if x=='video games' or x=='social_networking'\n",
    "                                             else ('outdoor' if x=='fishing' or x=='sports' or x=='yoga' or x=='camping'\n",
    "                                             else('movies' if x=='movies' or x=='acting' or x=='makeup'\n",
    "                                             else ('read/write' if x=='reading' or x=='writting' else x))))))\n",
    "\n",
    "dummies=pd.get_dummies(data['interests'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('interests',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['dropped_out'],drop_first=True)\n",
    "dummies=dummies.rename(columns={'yes':'dropped_out_yes'})\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('dropped_out',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['location_preference'],drop_first=True)\n",
    "dummies=dummies.rename(columns={'same city':'location_same_city','same state':'location_same_state'})\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('location_preference',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['location']=data['location'].apply(lambda x: x.split(', ')[0])\n",
    "# top_20_locations=data['location'].value_counts()[:20]\n",
    "# top_20_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "san francisco     911\n",
       "location_other    252\n",
       "oakland           250\n",
       "berkeley          146\n",
       "san mateo         56 \n",
       "palo alto         45 \n",
       "san rafael        35 \n",
       "alameda           35 \n",
       "san leandro       33 \n",
       "redwood city      28 \n",
       "daly city         27 \n",
       "emeryville        27 \n",
       "walnut creek      24 \n",
       "hayward           21 \n",
       "pacifica          18 \n",
       "el cerrito        17 \n",
       "menlo park        16 \n",
       "burlingame        15 \n",
       "martinez          15 \n",
       "benicia           15 \n",
       "richmond          15 \n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['location']=data['location'].apply(lambda x: x.split(', ')[0])\n",
    "top_20_locations=['san francisco','oakland','berkeley','san mateo','palo alto','san rafael','alameda','san leandro','redwood city','emeryville','daly city','walnut creek','hayward','pacifica','el cerrito','menlo park','martinez','benicia','richmond','burlingame']\n",
    "data['location']=data['location'].apply(lambda x: x if x in top_20_locations else \"location_other\")\n",
    "data['location'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['location'])\n",
    "dummies = dummies[['benicia', 'berkeley', 'burlingame', 'daly city', 'el cerrito',\n",
    "       'emeryville', 'hayward', 'location_other', 'martinez', 'menlo park',\n",
    "       'oakland', 'pacifica', 'palo alto', 'redwood city', 'richmond',\n",
    "       'san francisco', 'san leandro', 'san mateo', 'san rafael',\n",
    "       'walnut creek']]\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('location',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benicia</th>\n",
       "      <th>berkeley</th>\n",
       "      <th>burlingame</th>\n",
       "      <th>daly city</th>\n",
       "      <th>el cerrito</th>\n",
       "      <th>emeryville</th>\n",
       "      <th>hayward</th>\n",
       "      <th>location_other</th>\n",
       "      <th>martinez</th>\n",
       "      <th>menlo park</th>\n",
       "      <th>oakland</th>\n",
       "      <th>pacifica</th>\n",
       "      <th>palo alto</th>\n",
       "      <th>redwood city</th>\n",
       "      <th>richmond</th>\n",
       "      <th>san francisco</th>\n",
       "      <th>san leandro</th>\n",
       "      <th>san mateo</th>\n",
       "      <th>san rafael</th>\n",
       "      <th>walnut creek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      benicia  berkeley  burlingame  daly city  el cerrito  emeryville  \\\n",
       "0     0        0         0           0          0           0            \n",
       "1     0        0         0           0          0           0            \n",
       "2     0        0         0           0          0           0            \n",
       "3     0        0         0           1          0           0            \n",
       "4     0        0         0           0          0           0            \n",
       "...  ..       ..        ..          ..         ..          ..            \n",
       "1996  0        0         0           0          0           0            \n",
       "1997  0        0         0           0          0           0            \n",
       "1998  0        0         0           0          0           0            \n",
       "1999  0        0         0           0          0           0            \n",
       "2000  0        0         0           0          0           0            \n",
       "\n",
       "      hayward  location_other  martinez  menlo park  oakland  pacifica  \\\n",
       "0     0        0               0         0           1        0          \n",
       "1     0        1               0         0           0        0          \n",
       "2     0        0               0         0           1        0          \n",
       "3     0        0               0         0           0        0          \n",
       "4     0        0               0         0           1        0          \n",
       "...  ..       ..              ..        ..          ..       ..          \n",
       "1996  0        0               0         0           1        0          \n",
       "1997  0        0               0         0           0        0          \n",
       "1998  0        1               0         0           0        0          \n",
       "1999  0        0               0         0           0        0          \n",
       "2000  0        0               0         0           1        0          \n",
       "\n",
       "      palo alto  redwood city  richmond  san francisco  san leandro  \\\n",
       "0     0          0             0         0              0             \n",
       "1     0          0             0         0              0             \n",
       "2     0          0             0         0              0             \n",
       "3     0          0             0         0              0             \n",
       "4     0          0             0         0              0             \n",
       "...  ..         ..            ..        ..             ..             \n",
       "1996  0          0             0         0              0             \n",
       "1997  0          0             0         1              0             \n",
       "1998  0          0             0         0              0             \n",
       "1999  0          0             0         1              0             \n",
       "2000  0          0             0         0              0             \n",
       "\n",
       "      san mateo  san rafael  walnut creek  \n",
       "0     0          0           0             \n",
       "1     0          0           0             \n",
       "2     0          0           0             \n",
       "3     0          0           0             \n",
       "4     0          0           0             \n",
       "...  ..         ..          ..             \n",
       "1996  0          0           0             \n",
       "1997  0          0           0             \n",
       "1998  0          0           0             \n",
       "1999  0          0           0             \n",
       "2000  0          0           0             \n",
       "\n",
       "[2001 rows x 20 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-f420f661be47>:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col_name[j]][i]=results[j]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pet_encoding(x):\n",
    "    if x=='likes dogs and likes cats':\n",
    "        return [1,1,0,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs':\n",
    "        return [1,0,0,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs and has cats':\n",
    "        return [1,1,0,1,0,0]\n",
    "    \n",
    "    elif x=='has dogs':\n",
    "        return [1,0,1,0,0,0]\n",
    "    \n",
    "    elif x=='has dogs and likes cats':\n",
    "        return [1,1,1,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs and dislikes cats':\n",
    "        return [1,0,0,0,0,1]\n",
    "    \n",
    "    elif x=='has dogs and has cats':\n",
    "        return [1,1,1,1,0,0]\n",
    "    \n",
    "    elif x=='has cats':\n",
    "        return [0,1,0,1,0,0]\n",
    "    \n",
    "    elif x=='likes cats':\n",
    "        return [0,1,0,0,0,0]\n",
    "    \n",
    "    elif x=='has dogs and dislikes cats':\n",
    "        return [1,0,1,0,0,1]\n",
    "    \n",
    "    elif x=='dislikes dogs and dislikes cats':\n",
    "        return [0,0,0,0,1,1]\n",
    "    \n",
    "    elif x=='dislikes dogs and likes cats':\n",
    "        return [0,1,0,0,1,0]\n",
    "    \n",
    "    elif x=='dislikes dogs':\n",
    "        return [0,0,0,0,1,0]\n",
    "    \n",
    "    elif x=='dislikes dogs and has cats':\n",
    "        return [0,1,0,1,1,0]\n",
    "    \n",
    "    elif x=='dislikes cats':\n",
    "        return [0,0,0,0,0,1]\n",
    "    else:\n",
    "        print('Error'+x)\n",
    "\n",
    "col_name=['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']\n",
    "for i in col_name:\n",
    "    data[i]=''\n",
    "for i in range(len(data['pets'])):\n",
    "    \n",
    "    #data[['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']][i]=pet_encoding(data['pets'][i])\n",
    "    results=pet_encoding(data['pets'][i])\n",
    "   \n",
    "    for j in range(len(results)):\n",
    "        data[col_name[j]][i]=results[j]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#data[['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']]=pet_encoding(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-e0369c7f657b>:12: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akadk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akadk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "import pickle \n",
    "# Setting options\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "wordlist = words.words()\n",
    "\n",
    "# Function for removing punctuation\n",
    "def drop_punc(my_text):\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# Function for making all text lowercase\n",
    "def lower(my_text):\n",
    "    clean_text = my_text.lower()\n",
    "    return clean_text\n",
    "\n",
    "# Function for removing all numbers\n",
    "def remove_numbers(my_text):\n",
    "    clean_text = re.sub('\\w*\\d\\w*', '', my_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# Function for removing emojis\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in stop_words])\n",
    "\n",
    "# Function for stripping whitespace\n",
    "def my_strip(my_text):\n",
    "    try: return my_text.strip()\n",
    "    except Exception as e: return None\n",
    "    \n",
    "\n",
    "my_stop_words = ['mmmmm','im']\n",
    "\n",
    "# Function for removing my stop words\n",
    "def remove_my_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in my_stop_words])\n",
    "\n",
    "\n",
    "## to determine the language of the bio\n",
    "\n",
    "# Function to detect english\n",
    "def is_english(my_text):\n",
    "    if my_text is None:\n",
    "        return my_text\n",
    "    text_list = my_text.split()\n",
    "    english = 0\n",
    "    non_english = 0\n",
    "    for word in text_list:\n",
    "        if word not in wordlist:\n",
    "            non_english += 1\n",
    "        else:\n",
    "            english += 1\n",
    "    if english > 0.25*non_english:\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "    \n",
    "test = data.sample(50)\n",
    "\n",
    "data['bio_cleaned'] = data['bio'].apply(lower).apply(drop_punc).apply(remove_numbers).apply(deEmojify)\n",
    "data['bio_cleaned'] = data['bio_cleaned'].apply(remove_stop).apply(remove_my_stop)\n",
    "data['bio_cleaned'] = data['bio_cleaned'].str.strip()\n",
    "\n",
    "# Pickling the tokenized words and bigrams\n",
    "with open(\"clean_data.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(data, fp)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import os\n",
    "\n",
    "\n",
    "corpus = pd.read_pickle('clean_data.pkl')\n",
    "corpus.shape\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "\n",
    "df = pd.read_pickle('clean_data.pkl')\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "doc_word = vectorizer.fit_transform(df['bio_cleaned'])\n",
    "\n",
    "\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "doc_word_tf_idf = tf_idf.fit_transform(df['bio_cleaned'])\n",
    "\n",
    "word2vec = models.Word2Vec(df['bio_cleaned'], vector_size=100, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "num_topics = 15\n",
    "\n",
    "lsa = TruncatedSVD(num_topics)\n",
    "doc_topic = lsa.fit_transform(doc_word_tf_idf)\n",
    "\n",
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = list(range(num_topics)),\n",
    "             columns = vectorizer.get_feature_names())\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "\n",
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = df['bio_cleaned'],\n",
    "             columns = ['1', '2', '3', '4', '5', \n",
    "                        '6', '7', '8', '9', '10', \n",
    "                        '11', '12', '13', '14', '15'])\n",
    "\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model = NMF(num_topics, random_state=42)\n",
    "doc_topic_nmf = nmf_model.fit_transform(doc_word)\n",
    "\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model_tf_idf = NMF(num_topics, random_state=42)\n",
    "doc_topic_nmf_tf_idf = nmf_model_tf_idf.fit_transform(doc_word_tf_idf)\n",
    "\n",
    "\n",
    "Vt_nmf = pd.DataFrame(doc_topic_nmf.round(5),\n",
    "             index = df['bio_cleaned'],\n",
    "             columns = ['Love', 'Travel', 'Friends', 'Fun', 'Humor', 'Music'])\n",
    "\n",
    "\n",
    "topics = ['Knowing each other','Explore','Full of life','Fun Partner','Easy Going','Meeting new people']\n",
    "def nmf_predict(vectorizer, model, text):\n",
    "    text_vect = vectorizer.transform([text])\n",
    "    result = model.transform(text_vect)\n",
    "    return topics[np.argmax(result)]\n",
    "\n",
    "df_w_labels = df.copy()\n",
    "\n",
    "\n",
    "labels = []\n",
    "for index, row in df.iterrows():\n",
    "    x = nmf_predict(vectorizer=tf_idf, model=nmf_model_tf_idf, text = row['bio_cleaned'])\n",
    "    labels.append(x)\n",
    "    \n",
    "    \n",
    "df_w_labels['labels'] = labels\n",
    "\n",
    "\n",
    "df_w_labels.to_pickle('df_w_labels.pkl')\n",
    "\n",
    "data['bio_labels']=df_w_labels['labels'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['bio_labels'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('bio_labels',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'height', 'pets', 'language', 'new_languages', 'education_level',\n",
       "       'bio', 'other_interests', '31_40', '41_50', '50+', 'drinks_often',\n",
       "       'drinks_rarely', 'drinks_socially', 'drugs_sometimes',\n",
       "       'orientation_gay', 'orientation_straight',\n",
       "       'banking / financial / real estate', 'education / academia',\n",
       "       'entertainment / media', 'executive / management', 'medicine / health',\n",
       "       'other_job', 'sales / marketing / biz dev',\n",
       "       'science / tech / engineering', 'student', 'smokes_sometimes',\n",
       "       'smokes_yes', 'athletic', 'fat', 'fit', 'skinny', 'single', 'astronomy',\n",
       "       'collectibles', 'cooking', 'dancing', 'food', 'game_video', 'gardening',\n",
       "       'movies', 'music', 'organising events', 'outdoor', 'photography',\n",
       "       'politics', 'read/write', 'singing', 'studying', 'travelling',\n",
       "       'dropped_out_yes', 'location_same_city', 'location_same_state',\n",
       "       'benicia', 'berkeley', 'burlingame', 'daly city', 'el cerrito',\n",
       "       'emeryville', 'hayward', 'location_other', 'martinez', 'menlo park',\n",
       "       'oakland', 'pacifica', 'palo alto', 'redwood city', 'richmond',\n",
       "       'san francisco', 'san leandro', 'san mateo', 'san rafael',\n",
       "       'walnut creek', 'likes dogs', 'likes cats', 'has dog', 'has cat',\n",
       "       'dislikes dogs', 'dislike cats', 'bio_cleaned', 'Explore',\n",
       "       'Full of life', 'Fun Partner', 'Knowing each other',\n",
       "       'Meeting new people'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['sex','height','pets','language','new_languages','bio','other_interests','bio_cleaned'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['education_level', 'drinks_often', 'drinks_rarely', 'drinks_socially',\n",
       "       'drugs_sometimes', 'orientation_gay', 'orientation_straight',\n",
       "       'banking / financial / real estate', 'education / academia',\n",
       "       'entertainment / media', 'executive / management', 'medicine / health',\n",
       "       'other_job', 'sales / marketing / biz dev',\n",
       "       'science / tech / engineering', 'student', 'likes dogs', 'likes cats',\n",
       "       'has dog', 'has cat', 'dislikes dogs', 'dislike cats',\n",
       "       'smokes_sometimes', 'smokes_yes', 'athletic', 'fat', 'fit', 'skinny',\n",
       "       'dropped_out_yes', 'location_same_city', 'location_same_state',\n",
       "       'single', 'astronomy', 'collectibles', 'cooking', 'dancing', 'food',\n",
       "       'game_video', 'gardening', 'movies', 'music', 'organising events',\n",
       "       'outdoor', 'photography', 'politics', 'read/write', 'singing',\n",
       "       'studying', 'travelling', 'benicia', 'berkeley', 'burlingame',\n",
       "       'daly city', 'el cerrito', 'emeryville', 'hayward', 'location_other',\n",
       "       'martinez', 'menlo park', 'oakland', 'pacifica', 'palo alto',\n",
       "       'redwood city', 'richmond', 'san francisco', 'san leandro', 'san mateo',\n",
       "       'san rafael', 'walnut creek', 'Explore', 'Full of life', 'Fun Partner',\n",
       "       'Knowing each other', 'Meeting new people', '31_40', '41_50', '50+',\n",
       "       'cluster_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove this\n",
    "train=pd.read_csv('PreprocessedData_withClusterLables.csv')\n",
    "train.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_list=train.drop('cluster_labels',axis=1).columns\n",
    "col_list=['education_level', 'drinks_often', 'drinks_rarely', 'drinks_socially',\n",
    "       'drugs_sometimes', 'orientation_gay', 'orientation_straight',\n",
    "       'banking / financial / real estate', 'education / academia',\n",
    "       'entertainment / media', 'executive / management', 'medicine / health',\n",
    "       'other_job', 'sales / marketing / biz dev',\n",
    "       'science / tech / engineering', 'student', 'likes dogs', 'likes cats',\n",
    "       'has dog', 'has cat', 'dislikes dogs', 'dislike cats',\n",
    "       'smokes_sometimes', 'smokes_yes', 'athletic', 'fat', 'fit', 'skinny',\n",
    "       'dropped_out_yes', 'location_same_city', 'location_same_state',\n",
    "       'single', 'astronomy', 'collectibles', 'cooking', 'dancing', 'food',\n",
    "       'game_video', 'gardening', 'movies', 'music', 'organising events',\n",
    "       'outdoor', 'photography', 'politics', 'read/write', 'singing',\n",
    "       'studying', 'travelling', 'benicia', 'berkeley', 'burlingame',\n",
    "       'daly city', 'el cerrito', 'emeryville', 'hayward', 'location_other',\n",
    "       'martinez', 'menlo park', 'oakland', 'pacifica', 'palo alto',\n",
    "       'redwood city', 'richmond', 'san francisco', 'san leandro', 'san mateo',\n",
    "       'san rafael', 'walnut creek', 'Explore', 'Full of life', 'Fun Partner',\n",
    "       'Knowing each other', 'Meeting new people', '31_40', '41_50', '50+']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=data\n",
    "data2=data[col_list]\n",
    "len(data2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('logistic_regression_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# remove below test\n",
    "test=data.iloc[5,:]\n",
    "test=np.array(test).reshape((1,-1))\n",
    "test\n",
    "# Calculate the accuracy score and predict target values\n",
    "# score = pickle_model.score(Xtest, Ytest)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_number=model.predict(test)\n",
    "cluster_number=int(cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing raw data with cluster labels for reccommendations\n",
    "recc_data=pd.read_csv('Rawdata_withClusterLabels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       3\n",
       "4       3\n",
       "5       3\n",
       "20      3\n",
       "25      3\n",
       "       ..\n",
       "1980    3\n",
       "1990    3\n",
       "1996    3\n",
       "1997    3\n",
       "2000    3\n",
       "Name: cluster_labels, Length: 549, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# check gender and orientation and according to that send recommendation from cluster_number\n",
    "\n",
    "recc_data=recc_data.loc[recc_data['cluster_labels']==cluster_number]\n",
    "recc_data['cluster_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
