{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1_R0EORJWLSP"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WUJIgsyxWLSR"
   },
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "data=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LqvNSVH4WLSS"
   },
   "outputs": [],
   "source": [
    "\n",
    "data=data.drop(['username','user_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L_EVHbuxWLST",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['age']=data['age'].apply(lambda x: 58 if x>58 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X2ddRriwWLSU"
   },
   "outputs": [],
   "source": [
    "data['drinks']=data['drinks'].apply(lambda x:'rarely' if x=='very often' else ('often' if x=='desperately' else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SAHGh2QyWLSV"
   },
   "outputs": [],
   "source": [
    "\n",
    "data['drugs']=data['drugs'].apply(lambda x: 'sometimes' if x=='often' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hn6LPaGZWLSV"
   },
   "outputs": [],
   "source": [
    "data.drop('sex',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IOK5lsRrWLSW",
    "outputId": "ffb2e53d-e97e-41d5-c019-676874c210cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'status', 'orientation', 'drinks', 'drugs', 'height', 'job',\n",
       "       'location', 'pets', 'smokes', 'language', 'new_languages',\n",
       "       'body_profile', 'education_level', 'dropped_out', 'bio', 'interests',\n",
       "       'other_interests', 'location_preference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Em8HJdIQWLSX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data[['drinks','drugs','orientation']],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n",
    "data=data.drop(['drinks','drugs','orientation'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XSR8URTJWLSY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data['job']=data['job'].apply(lambda x:'science / tech / engineering' if x=='computer / hardware / software'\n",
    "                                       else('other' if (x=='rather not say' or x=='unemployed' or x=='retired' or x=='military' or\n",
    "                                                       x=='transportation' or x=='political / government' or x=='clerical / administrative'\n",
    "                                                       or x=='hospitality / travel'  or x=='construction / craftsmanship' or x=='law / legal services')\n",
    "                                           else x)\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1lD2YKNzWLSZ"
   },
   "outputs": [],
   "source": [
    "data['job']=data['job'].apply(lambda x:'other_job' if x=='other' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WwYzkwlSWLSa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dummies=pd.get_dummies(data['job'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rlASZ29eWLSa"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('job',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FUp5uQvcWLSb"
   },
   "outputs": [],
   "source": [
    "data['location']=data['location'].apply(lambda x: x.split(', ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p7bxOjHcWLSb",
    "outputId": "a8bce39e-eb15-47cf-ab2a-52b789c8d692"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.likes dogs\\n2.likes cats\\n3.has dog\\n4.has cat\\n5.dislike dogs\\n6.dislike cats\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handling pets column\n",
    "data['pets'].value_counts()\n",
    "#can be summerised into 6 categories\n",
    "'''\n",
    "1.likes dogs\n",
    "2.likes cats\n",
    "3.has dog\n",
    "4.has cat\n",
    "5.dislike dogs\n",
    "6.dislike cats\n",
    "'''\n",
    "#those who has dogs/cats obivously likes dogs/cats , so has dog/cat would also make 1 under likes dogs/cats category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y0sD0jnBWLSc",
    "outputId": "3ddbc971-3012-429f-d523-5046c47be9eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-f420f661be47>:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col_name[j]][i]=results[j]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pet_encoding(x):\n",
    "    if x=='likes dogs and likes cats':\n",
    "        return [1,1,0,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs':\n",
    "        return [1,0,0,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs and has cats':\n",
    "        return [1,1,0,1,0,0]\n",
    "    \n",
    "    elif x=='has dogs':\n",
    "        return [1,0,1,0,0,0]\n",
    "    \n",
    "    elif x=='has dogs and likes cats':\n",
    "        return [1,1,1,0,0,0]\n",
    "    \n",
    "    elif x=='likes dogs and dislikes cats':\n",
    "        return [1,0,0,0,0,1]\n",
    "    \n",
    "    elif x=='has dogs and has cats':\n",
    "        return [1,1,1,1,0,0]\n",
    "    \n",
    "    elif x=='has cats':\n",
    "        return [0,1,0,1,0,0]\n",
    "    \n",
    "    elif x=='likes cats':\n",
    "        return [0,1,0,0,0,0]\n",
    "    \n",
    "    elif x=='has dogs and dislikes cats':\n",
    "        return [1,0,1,0,0,1]\n",
    "    \n",
    "    elif x=='dislikes dogs and dislikes cats':\n",
    "        return [0,0,0,0,1,1]\n",
    "    \n",
    "    elif x=='dislikes dogs and likes cats':\n",
    "        return [0,1,0,0,1,0]\n",
    "    \n",
    "    elif x=='dislikes dogs':\n",
    "        return [0,0,0,0,1,0]\n",
    "    \n",
    "    elif x=='dislikes dogs and has cats':\n",
    "        return [0,1,0,1,1,0]\n",
    "    \n",
    "    elif x=='dislikes cats':\n",
    "        return [0,0,0,0,0,1]\n",
    "    else:\n",
    "        print('Error'+x)\n",
    "\n",
    "col_name=['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']\n",
    "for i in col_name:\n",
    "    data[i]=''\n",
    "for i in range(len(data['pets'])):\n",
    "    \n",
    "    #data[['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']][i]=pet_encoding(data['pets'][i])\n",
    "    results=pet_encoding(data['pets'][i])\n",
    "   \n",
    "    for j in range(len(results)):\n",
    "        data[col_name[j]][i]=results[j]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#data[['likes dogs','likes cats','has dog','has cat','dislikes dogs','dislike cats']]=pet_encoding(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dKfn3j2sWLSf"
   },
   "outputs": [],
   "source": [
    "#dropping pets column\n",
    "data.drop('pets',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a2AL-q-8WLSg"
   },
   "outputs": [],
   "source": [
    "#merging \n",
    "data['smokes']=data['smokes'].apply(lambda x: 'sometimes' if x=='when drinking' or x=='trying to quit' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7C6aFKTrWLSg"
   },
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['smokes'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1jru-xHfWLSh"
   },
   "outputs": [],
   "source": [
    "dummies=dummies.rename(columns={'sometimes':'smokes_sometimes','yes':'smokes_yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hqwPanqwWLSi"
   },
   "outputs": [],
   "source": [
    "\n",
    "data=pd.concat([data,dummies],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "znhKoJh0WLSi"
   },
   "outputs": [],
   "source": [
    "data.drop('smokes',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "93qKxl5MWLSj"
   },
   "outputs": [],
   "source": [
    "#merging body profile to levels\n",
    "data['body_profile']=data['body_profile'].apply(lambda x:'skinny' if (x=='skinny' or x=='thin' or x=='used up')\n",
    "                                                       else('athletic' if(x=='athletic' or x=='jacked') \n",
    "                                                       else('fit' if (x=='average' or x=='fit')\n",
    "                                                       else('above average' if (x=='curvy' or x=='full figured' or x=='a little extra')else 'fat'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vcEiXRGOWLSj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['body_profile'],drop_first=True)\n",
    "data=pd.concat([data,dummies],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ck4ZuL2ZWLSk"
   },
   "outputs": [],
   "source": [
    "#handling column name - dropped out\n",
    "dummies=pd.get_dummies(data['dropped_out'],drop_first=True)\n",
    "dummies=dummies.rename(columns={'yes':'dropped_out_yes'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "j9nSoOteWLSl"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "q5iK1OgkWLSm"
   },
   "outputs": [],
   "source": [
    "data.drop('dropped_out',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "G6WofktAWLSm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#handling location preference \n",
    "dummies=pd.get_dummies(data['location_preference'],drop_first=True)\n",
    "dummies=dummies.rename(columns={'same city':'location_same_city','same state':'location_same_state'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lzQvQidbWLSn"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('location_preference',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zV5kzao7WLSo"
   },
   "outputs": [],
   "source": [
    "#merging status\n",
    "data['status']=data['status'].apply(lambda x:'single' if x=='single' else 'Status_other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "hlo10nDAWLSo",
    "outputId": "308a8650-e20e-4481-f017-477a84a207bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#handlling status column\n",
    "dummies=pd.get_dummies(data['status'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GCJpMPKNWLSp"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('status',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_S45vVOdWLSp"
   },
   "outputs": [],
   "source": [
    "#merging interest\n",
    "data['interests']=data['interests'].apply(lambda x:'music' if x=='music' or x=='singinig' or x=='instruments' or x=='dance'\n",
    "                                             else('artist' if x=='calligraphy' or x=='diy' or x=='painting' or x=='sketching' or x=='designing' or x=='craft' \n",
    "                                             else('game_video' if x=='video games' or x=='social_networking'\n",
    "                                             else ('outdoor' if x=='fishing' or x=='sports' or x=='yoga' or x=='camping'\n",
    "                                             else('movies' if x=='movies' or x=='acting' or x=='makeup'\n",
    "                                             else ('read/write' if x=='reading' or x=='writting' else x))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EL6I-HUqWLSq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['interests'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PIlWyZ3sWLSr"
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop('interests',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Sz-lb8j-WLSs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#language,other_interest and new lang not including in project ,, body profile already encoded\n",
    "data.drop(['language','new_languages','body_profile','other_interests'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Qkh11-3GWLSs"
   },
   "outputs": [],
   "source": [
    "#handling location col\n",
    "#top 20 locations contributing to 87% of total count so other than top 20 will be \"other\"\n",
    "top_20_locations=data['location'].value_counts()[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DvY1SrT3WLSt"
   },
   "outputs": [],
   "source": [
    "data['location']=data['location'].apply(lambda x: x if x in top_20_locations else \"location_other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Ap9mgdJ0WLSu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting dummies of location columns\n",
    "dummies=pd.get_dummies(data['location'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zoG4hcYNWLSu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "#dropping other interest and location colum\n",
    "data.drop(['location'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL_FEYcTWjZT"
   },
   "source": [
    "# NLP Preprocessing on Bio column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gAVB_ecdWLSv",
    "outputId": "c8224c00-fb45-48ee-9281-39aaa3dbba8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-554c2044b897>:12: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Siddhant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Siddhant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "import pickle \n",
    "# Setting options\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "wordlist = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "tacS5akpWLSw"
   },
   "outputs": [],
   "source": [
    "# Function for removing punctuation\n",
    "def drop_punc(my_text):\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gzdVnCqmWLSx"
   },
   "outputs": [],
   "source": [
    "# Function for making all text lowercase\n",
    "def lower(my_text):\n",
    "    clean_text = my_text.lower()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "itqdvOwjWLSy"
   },
   "outputs": [],
   "source": [
    "# Function for removing all numbers\n",
    "def remove_numbers(my_text):\n",
    "    clean_text = re.sub('\\w*\\d\\w*', '', my_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vJk0E4QAWLSy"
   },
   "outputs": [],
   "source": [
    "# Function for removing emojis\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hcJZ_KFfWLSz"
   },
   "outputs": [],
   "source": [
    "# Function for removing stop words\n",
    "def remove_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "wA_e-Q4uWLSz"
   },
   "outputs": [],
   "source": [
    "# Function for stripping whitespace\n",
    "def my_strip(my_text):\n",
    "    try: return my_text.strip()\n",
    "    except Exception as e: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ZUoI38GzWLS0"
   },
   "outputs": [],
   "source": [
    "# Curated list of additional stop-words for this project\n",
    "my_stop_words = ['mmmmm','im']\n",
    "\n",
    "# Function for removing my stop words\n",
    "def remove_my_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in my_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "BvTYvm3lWLS1"
   },
   "outputs": [],
   "source": [
    "## to determine the language of the bio\n",
    "\n",
    "# Function to detect english\n",
    "def is_english(my_text):\n",
    "    if my_text is None:\n",
    "        return my_text\n",
    "    text_list = my_text.split()\n",
    "    english = 0\n",
    "    non_english = 0\n",
    "    for word in text_list:\n",
    "        if word not in wordlist:\n",
    "            non_english += 1\n",
    "        else:\n",
    "            english += 1\n",
    "    if english > 0.25*non_english:\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "xcCdP2TmWLS2"
   },
   "outputs": [],
   "source": [
    "test = data.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Ydtv14jOWLS2"
   },
   "outputs": [],
   "source": [
    "data['bio_cleaned'] = data['bio'].apply(lower).apply(drop_punc).apply(remove_numbers).apply(deEmojify)\n",
    "data['bio_cleaned'] = data['bio_cleaned'].apply(remove_stop).apply(remove_my_stop)\n",
    "data['bio_cleaned'] = data['bio_cleaned'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "KlRD3Xn3WLS3"
   },
   "outputs": [],
   "source": [
    "# Pickling the tokenized words and bigrams\n",
    "#with open(\"clean_data.pkl\", \"wb\") as fp:\n",
    " #   pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4674k16WLS5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ZyOlRkjeWLS5",
    "outputId": "9565bb79-5ac8-4152-d0ce-7d03aabe0e75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siddhant\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Clustering (Vectorize and cluster with Word2vec)\n",
    "# imports\n",
    "# !pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "bWsd50mwWLS7",
    "outputId": "e626113e-4379-4fd3-db44-4db237dce332"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 80)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "corpus = pd.read_pickle('clean_data.pkl')\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6M24ZlH6WLS7",
    "outputId": "4979bfc9-f858-4835-848b-a500f05efb21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bottom line love life work hard love hard earth drama free plan keep way want know ask woman loves women sometimes feel like nut sometimes femme side butch side love trying new things spend day helping people check list going love making people laugh open meeting new people dating point taking care heart rushing anything value communication open minded                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "1    straightforward genuine fun loving serious profile may make grounded guy bend backwards care pushover love simple things like night little spoon grabbing bite eat friends watching tv vegging friends decide something love active outdoors enjoying sun fresh air well filipino german hybrid identify pacific islander caucasian asian best way stick foot mouth calling asian also call hapa born raised california dark blonde hair knew right green eyes svelte dancer build enjoys cocktails non smoker doesnt drugs bottom include matter grand scheme relationship like topping sassy straightforward intelligent earth fun loving driven tactile sexual sensitive even though try hide genuine responsible loyal caring friendly cordial like people talk match ideal respond assume response conveys interest monosyllabic responses good indications interested looking fun many different forms platonic friends friends benefits dating relationship enjoy things cooking hanging friends watching movies dining social libating bars clubs outdoor activities road trips cuddling especially cuddling great big spoon giving massages bowling gourd art many things looking guys love life enjoy social look guy born male fluent english ages anyone really least taller american type looks anything goes long looks good find attractive slim muscular relative fitness level comparison mannerisms middle road masculine non smokers includes marijuana hookah okay drug users alcohol acceptable good kisser big spoon type take charge guy intelligent humorous affectionate highly sexual vers top total top deal breakers guys lie thier age stupid things like flakey guys guys smoke cigarettes marijuana illegal drugs guys women clothing guys wear makeup eyeliner mascara consealer gesticulate bad kissers bottom top passive general guys use girl refer another gay male among unwanted traits get gist please close hour car maybe hours looking pen pals please waste time messaging around bay area\n",
       "2    yummy tacosss yoga love life part content person easy pleaser free spirit home body want good things world live every day normally get long people active favorite color pink prissy generally well people amount girls associate limited okay like cars ended friend wanted wanted share entertainment site mean use friends men life first bf second chinchilla nicco work go school spend much time world sounds okay message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "3    stealth geek special mix technical obsession enough social awareness look like something revenge nerds hip knew self image would like project one somebody far cooler sort would love covered tattoos save fact explain portrait crazed charles bronson grandkids bicycle fixed gear folds half instead sport gigantic mustache hopes mistaken self confidence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4    whisper wind weaved curls crashing waves tide spit onto sands time left demise                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "Name: bio_cleaned, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check it out\n",
    "corpus['bio_cleaned'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GtNjpLG_WLS8"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8uI9EVnDWLS9"
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_pickle('clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "DHFfaqp0WLS-",
    "outputId": "a2160df1-ea5b-4307-d5af-1a5791f11e18"
   },
   "outputs": [],
   "source": [
    "# Vectorize the words with CountVectorizer. This takes a parameter 'stopwords', which I have left out \n",
    "# because I took care of stopwords in my cleaning phase \n",
    "\n",
    "# CountVectorizer().fit_transform returns a matrix where rows correspond to observations (documents) and \n",
    "# columns correspond to the counts of words in each observation. There are as many columns as number of unique \n",
    "# words in all documents\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "doc_word = vectorizer.fit_transform(df['bio_cleaned'])\n",
    "#doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "CV56p0kRWLS-",
    "outputId": "7a97fc80-57c1-4615-be58-f861a2dcda35"
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer().fit_transform does something similar, but it computes the term frequency of a word with a document\n",
    "# divided by the frequency of that word accross all documents. \n",
    "# Essentially, it is supposed to pick out words that are unique to a document as opposed to words that simply\n",
    "# show up in many documents\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "doc_word_tf_idf = tf_idf.fit_transform(df['bio_cleaned'])\n",
    "#doc_word_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "TnbZK_xPWLS_"
   },
   "outputs": [],
   "source": [
    "word2vec = models.Word2Vec(df['bio_cleaned'], vector_size=100, window=5, min_count=1, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "AdB_FM_GWLTA",
    "outputId": "db3531c3-6c23-4c75-f058-613040c13285"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0067713 , 0.00562123, 0.00473948, 0.00446345, 0.00398142,\n",
       "       0.00363887, 0.00346515, 0.00330676, 0.00306257, 0.00293037,\n",
       "       0.00285923, 0.00279225, 0.00269117, 0.00260621, 0.0025406 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use LSA to reduce dimensionality. I am going to guess we should use 15 TOPICS\n",
    "\n",
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "# Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "\n",
    "# in here, we can either input \"doc_word\" or \"doc_word_tf_idf\" to used different vectorized forms of our words\n",
    "\n",
    "num_topics = 15\n",
    "\n",
    "lsa = TruncatedSVD(num_topics)\n",
    "doc_topic = lsa.fit_transform(doc_word_tf_idf)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "eHazHS3tWLTB",
    "outputId": "46fc2fa4-bbdf-41de-f9e3-d5b1f5c1705b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aau</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abd</th>\n",
       "      <th>abducted</th>\n",
       "      <th>aber</th>\n",
       "      <th>...</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zookeeper</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zoos</th>\n",
       "      <th>zu</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zynga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 14074 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa    aaa  aaaaand    aau  abandon  abandoned  abbey    abd  abducted  \\\n",
       "0   0.000  0.000  0.000    0.001  0.000    0.000      0.001  0.000  0.000      \n",
       "1   0.000 -0.000 -0.001    0.003 -0.000   -0.001     -0.000 -0.000  0.000      \n",
       "2   0.000 -0.001 -0.001   -0.001 -0.000   -0.002     -0.002 -0.001  0.000      \n",
       "3   0.001  0.000 -0.001   -0.001  0.000   -0.000      0.001  0.000 -0.000      \n",
       "4  -0.001 -0.001  0.001   -0.003 -0.000   -0.001      0.000 -0.000 -0.002      \n",
       "5  -0.001 -0.001 -0.001    0.002  0.000    0.001      0.000 -0.000 -0.000      \n",
       "6  -0.000 -0.001  0.001    0.005  0.000    0.000      0.001 -0.001  0.000      \n",
       "7  -0.000  0.001 -0.001   -0.003  0.000   -0.001     -0.001 -0.000 -0.000      \n",
       "8   0.000  0.000 -0.000    0.003  0.001    0.001     -0.000 -0.001 -0.000      \n",
       "9  -0.002 -0.001 -0.000   -0.002 -0.000    0.002      0.000 -0.000 -0.001      \n",
       "10  0.000  0.000 -0.000    0.001 -0.000   -0.003     -0.000 -0.003 -0.001      \n",
       "11  0.000 -0.000  0.000    0.004 -0.000    0.002      0.001  0.002 -0.001      \n",
       "12  0.000  0.001 -0.001   -0.003  0.000    0.001     -0.000  0.001 -0.001      \n",
       "13 -0.002 -0.001  0.002   -0.002 -0.001    0.000     -0.001  0.003  0.001      \n",
       "14 -0.001  0.001 -0.000    0.001 -0.000    0.000     -0.001  0.002  0.000      \n",
       "\n",
       "    aber  ...  zombies   zone    zoo  zookeeper  zooming   zoos     zu  zumba  \\\n",
       "0   0.0   ...  0.001    0.005  0.000  0.000      0.000    0.001  0.000  0.001   \n",
       "1  -0.0   ... -0.001   -0.009  0.000  0.001     -0.000   -0.002 -0.001 -0.000   \n",
       "2  -0.0   ... -0.001    0.002  0.000 -0.000     -0.000   -0.000 -0.001 -0.000   \n",
       "3   0.0   ...  0.002   -0.007  0.000  0.000      0.000   -0.001  0.000  0.001   \n",
       "4   0.0   ...  0.002   -0.004  0.001 -0.001      0.001    0.001  0.001  0.000   \n",
       "5   0.0   ... -0.003    0.017 -0.000  0.001     -0.002   -0.000  0.002  0.001   \n",
       "6   0.0   ...  0.001   -0.001  0.001 -0.001      0.001   -0.000  0.001 -0.000   \n",
       "7   0.0   ... -0.006    0.006  0.001  0.001      0.001   -0.002  0.000  0.001   \n",
       "8  -0.0   ...  0.000   -0.003 -0.001  0.000      0.003   -0.001 -0.001 -0.001   \n",
       "9   0.0   ... -0.003   -0.009 -0.000 -0.001     -0.001    0.002  0.002  0.001   \n",
       "10  0.0   ... -0.001   -0.000 -0.001 -0.000      0.000    0.002  0.000  0.001   \n",
       "11  0.0   ... -0.001   -0.008  0.000  0.002     -0.001    0.000  0.000  0.001   \n",
       "12 -0.0   ... -0.000    0.006 -0.002  0.001      0.002    0.002 -0.000  0.001   \n",
       "13  0.0   ... -0.007   -0.001 -0.000 -0.000      0.000    0.001  0.000  0.001   \n",
       "14 -0.0   ... -0.001    0.005 -0.001  0.000      0.001   -0.000 -0.001  0.000   \n",
       "\n",
       "    zurich  zynga  \n",
       "0   0.001   0.000  \n",
       "1  -0.000  -0.000  \n",
       "2   0.000  -0.001  \n",
       "3   0.000   0.000  \n",
       "4   0.001  -0.000  \n",
       "5   0.000  -0.000  \n",
       "6  -0.000   0.001  \n",
       "7  -0.001  -0.001  \n",
       "8  -0.001   0.001  \n",
       "9  -0.001  -0.000  \n",
       "10 -0.001   0.001  \n",
       "11  0.000   0.000  \n",
       "12  0.000  -0.000  \n",
       "13  0.001  -0.001  \n",
       "14 -0.001  -0.001  \n",
       "\n",
       "[15 rows x 14074 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = list(range(num_topics)),\n",
    "             columns = vectorizer.get_feature_names())\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "isq_gMUGWLTC"
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "h5fzZDj1WLTC",
    "outputId": "9546db78-b965-483c-e8af-f59c2d17af94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check out our topics!\n",
    "\n",
    "#display_topics(lsa, vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "eEa2Qa0_WLTE",
    "outputId": "df4e6dda-16f7-48e6-be76-6844879cf76b"
   },
   "outputs": [],
   "source": [
    "# Check out the Vt matrix, to see how each document falls along the topics:\n",
    "\n",
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = df['bio_cleaned'],\n",
    "             columns = ['1', '2', '3', '4', '5', \n",
    "                        '6', '7', '8', '9', '10', \n",
    "                        '11', '12', '13', '14', '15'])\n",
    "#Vt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vU3-DdTDWLTF"
   },
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rcdefaults()\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "gOlWYa86WLTG"
   },
   "outputs": [],
   "source": [
    "# Use NMF to reduce dimensionality. NMF might be better because it doesn't require that the topic axes are \n",
    "# orthogonal\n",
    "\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model = NMF(num_topics, random_state=42)\n",
    "doc_topic_nmf = nmf_model.fit_transform(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "XFWcSeqvWLTH"
   },
   "outputs": [],
   "source": [
    "# Copy of the above cell but using tf_idf instead of count-vectorizer\n",
    "\n",
    "num_topics = 6\n",
    "\n",
    "nmf_model_tf_idf = NMF(num_topics, random_state=42)\n",
    "doc_topic_nmf_tf_idf = nmf_model_tf_idf.fit_transform(doc_word_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "_MsK3WIpWLTH"
   },
   "outputs": [],
   "source": [
    "# Check out the Vt matrix, to see how each document falls along the topics:\n",
    "\n",
    "Vt_nmf = pd.DataFrame(doc_topic_nmf.round(5),\n",
    "             index = df['bio_cleaned'],\n",
    "             columns = ['Love', 'Travel', 'Friends', 'Fun', 'Humor', 'Music'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "169w6dwPWLTI",
    "outputId": "04fa2d61-fd82-4eb6-d1bd-f646f5742198"
   },
   "outputs": [],
   "source": [
    "# Let's look at the documents as represented within the topics\n",
    "#Vt_nmf.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "QBvo1Zf3WLTI",
    "outputId": "a190ea6e-6efb-4e3b-cf59-3d926bd475cb"
   },
   "outputs": [],
   "source": [
    "#display_topics(nmf_model_tf_idf, tf_idf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "MHpQngf9WLTJ"
   },
   "outputs": [],
   "source": [
    "# It seems like the best combination so far is NMF with TF-IDF.\n",
    "\n",
    "# (1) Knowing each other (2) Moved out (3) Full of life (4) Fun Partner (5) Easy Going (6) Meeting new people\n",
    "# Using NMF with CountVectorizer to classify documents\n",
    "# Let's make a function for this\n",
    "\n",
    "topics = ['Knowing each other','Explore','Full of life','Fun Partner','Easy Going','Meeting new people']\n",
    "def nmf_predict(vectorizer, model, text):\n",
    "    text_vect = vectorizer.transform([text])\n",
    "    result = model.transform(text_vect)\n",
    "    return topics[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "bMLqcJ3_WLTK",
    "outputId": "f2e4bc44-1448-4890-a229-790d06b8198b"
   },
   "outputs": [],
   "source": [
    "# Test it out!\n",
    "\n",
    "#test = 'Since start I knew you wanted to talk something, there was some burden like thing you are carrying within you. You say freely I would listen to it, dont hold any hesitations, trust me.'\n",
    "\n",
    "#nmf_predict(vectorizer=tf_idf, model=nmf_model_tf_idf, text = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Ehkj-Cx3WLTL"
   },
   "outputs": [],
   "source": [
    "# copy the dataframe\n",
    "df_w_labels = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "HQSQg1hRWLTL",
    "outputId": "cecd75f3-b6a6-4a04-8ab0-869c7dd01e54"
   },
   "outputs": [],
   "source": [
    "#df.drop(df.iloc[:, 0:22], axis=1)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "YDNoUsQuWLTM"
   },
   "outputs": [],
   "source": [
    "# get all the labels using the above function\n",
    "labels = []\n",
    "for index, row in df.iterrows():\n",
    "    x = nmf_predict(vectorizer=tf_idf, model=nmf_model_tf_idf, text = row['bio_cleaned'])\n",
    "    labels.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ojTDEeEgWLTM"
   },
   "outputs": [],
   "source": [
    "# assign the labels to a new column\n",
    "df_w_labels['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "rq4wZzj3WLTN",
    "outputId": "3e840414-ab74-4647-da28-0595d9c6a484"
   },
   "outputs": [],
   "source": [
    "# check it out!\n",
    "#df_w_labels.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "s1KIIFxnWLTO"
   },
   "outputs": [],
   "source": [
    "# Pickle the data so we can compare it with clusters\n",
    "#df_w_labels.to_pickle('df_w_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "LI7GN_4BWLTP",
    "outputId": "3bec89fa-6c13-4325-be6d-6dd0f7bcf7e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height', 'education_level', 'bio', 'drinks_often',\n",
       "       'drinks_rarely', 'drinks_socially', 'drugs_sometimes',\n",
       "       'orientation_gay', 'orientation_straight',\n",
       "       'banking / financial / real estate', 'education / academia',\n",
       "       'entertainment / media', 'executive / management', 'medicine / health',\n",
       "       'other_job', 'sales / marketing / biz dev',\n",
       "       'science / tech / engineering', 'student', 'likes dogs', 'likes cats',\n",
       "       'has dog', 'has cat', 'dislikes dogs', 'dislike cats',\n",
       "       'smokes_sometimes', 'smokes_yes', 'athletic', 'fat', 'fit', 'skinny',\n",
       "       'dropped_out_yes', 'location_same_city', 'location_same_state',\n",
       "       'single', 'astronomy', 'collectibles', 'cooking', 'dancing', 'food',\n",
       "       'game_video', 'gardening', 'movies', 'music', 'organising events',\n",
       "       'outdoor', 'photography', 'politics', 'read/write', 'singing',\n",
       "       'studying', 'travelling', 'benicia', 'berkeley', 'burlingame',\n",
       "       'daly city', 'el cerrito', 'emeryville', 'hayward', 'location_other',\n",
       "       'martinez', 'menlo park', 'oakland', 'pacifica', 'palo alto',\n",
       "       'redwood city', 'richmond', 'san francisco', 'san leandro', 'san mateo',\n",
       "       'san rafael', 'walnut creek', 'bio_cleaned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "-EV3ujV8WLTQ"
   },
   "outputs": [],
   "source": [
    "# removing bio and bio_cleaned from data and concatenating lables instead\n",
    "data.drop(['bio','bio_cleaned'],axis=1,inplace=True)\n",
    "data['bio_labels']=df_w_labels['labels'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "nRfXEFtEWLTQ",
    "outputId": "777bfbaa-8229-461c-ff8d-ce53d0cff0a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height', 'education_level', 'drinks_often', 'drinks_rarely',\n",
       "       'drinks_socially', 'drugs_sometimes', 'orientation_gay',\n",
       "       'orientation_straight', 'banking / financial / real estate',\n",
       "       'education / academia', 'entertainment / media',\n",
       "       'executive / management', 'medicine / health', 'other_job',\n",
       "       'sales / marketing / biz dev', 'science / tech / engineering',\n",
       "       'student', 'likes dogs', 'likes cats', 'has dog', 'has cat',\n",
       "       'dislikes dogs', 'dislike cats', 'smokes_sometimes', 'smokes_yes',\n",
       "       'athletic', 'fat', 'fit', 'skinny', 'dropped_out_yes',\n",
       "       'location_same_city', 'location_same_state', 'single', 'astronomy',\n",
       "       'collectibles', 'cooking', 'dancing', 'food', 'game_video', 'gardening',\n",
       "       'movies', 'music', 'organising events', 'outdoor', 'photography',\n",
       "       'politics', 'read/write', 'singing', 'studying', 'travelling',\n",
       "       'benicia', 'berkeley', 'burlingame', 'daly city', 'el cerrito',\n",
       "       'emeryville', 'hayward', 'location_other', 'martinez', 'menlo park',\n",
       "       'oakland', 'pacifica', 'palo alto', 'redwood city', 'richmond',\n",
       "       'san francisco', 'san leandro', 'san mateo', 'san rafael',\n",
       "       'walnut creek', 'bio_labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "MvQ_pZNaWLTR",
    "outputId": "b235c90a-0e23-4949-dcec-ff4caf26446c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Knowing each other    557\n",
       "Explore               349\n",
       "Easy Going            318\n",
       "Full of life          304\n",
       "Fun Partner           290\n",
       "Meeting new people    183\n",
       "Name: bio_labels, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['bio_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Z_rVlJSxWLTS",
    "outputId": "a4a88f79-a797-4079-fe20-552e0defb79e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explore</th>\n",
       "      <th>Full of life</th>\n",
       "      <th>Fun Partner</th>\n",
       "      <th>Knowing each other</th>\n",
       "      <th>Meeting new people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Explore  Full of life  Fun Partner  Knowing each other  \\\n",
       "0     0        1             0            0                    \n",
       "1     0        0             0            0                    \n",
       "2     0        0             0            1                    \n",
       "3     0        0             0            1                    \n",
       "4     1        0             0            0                    \n",
       "...  ..       ..            ..           ..                    \n",
       "1996  0        0             0            0                    \n",
       "1997  1        0             0            0                    \n",
       "1998  1        0             0            0                    \n",
       "1999  0        0             0            1                    \n",
       "2000  0        1             0            0                    \n",
       "\n",
       "      Meeting new people  \n",
       "0     0                   \n",
       "1     0                   \n",
       "2     0                   \n",
       "3     0                   \n",
       "4     0                   \n",
       "...  ..                   \n",
       "1996  0                   \n",
       "1997  0                   \n",
       "1998  0                   \n",
       "1999  0                   \n",
       "2000  0                   \n",
       "\n",
       "[2001 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding bio labels\n",
    "dummies=pd.get_dummies(data['bio_labels'],drop_first=True)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "p6pBCAGXWLTS",
    "outputId": "4459aa61-cc66-474d-9338-04ed2091854d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'height', 'education_level', 'drinks_often', 'drinks_rarely',\n",
       "       'drinks_socially', 'drugs_sometimes', 'orientation_gay',\n",
       "       'orientation_straight', 'banking / financial / real estate',\n",
       "       'education / academia', 'entertainment / media',\n",
       "       'executive / management', 'medicine / health', 'other_job',\n",
       "       'sales / marketing / biz dev', 'science / tech / engineering',\n",
       "       'student', 'likes dogs', 'likes cats', 'has dog', 'has cat',\n",
       "       'dislikes dogs', 'dislike cats', 'smokes_sometimes', 'smokes_yes',\n",
       "       'athletic', 'fat', 'fit', 'skinny', 'dropped_out_yes',\n",
       "       'location_same_city', 'location_same_state', 'single', 'astronomy',\n",
       "       'collectibles', 'cooking', 'dancing', 'food', 'game_video', 'gardening',\n",
       "       'movies', 'music', 'organising events', 'outdoor', 'photography',\n",
       "       'politics', 'read/write', 'singing', 'studying', 'travelling',\n",
       "       'benicia', 'berkeley', 'burlingame', 'daly city', 'el cerrito',\n",
       "       'emeryville', 'hayward', 'location_other', 'martinez', 'menlo park',\n",
       "       'oakland', 'pacifica', 'palo alto', 'redwood city', 'richmond',\n",
       "       'san francisco', 'san leandro', 'san mateo', 'san rafael',\n",
       "       'walnut creek', 'Explore', 'Full of life', 'Fun Partner',\n",
       "       'Knowing each other', 'Meeting new people'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "#dropping bio_labels\n",
    "data.drop(['bio_labels'],axis=1,inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "bk0GNfpPWLTT",
    "outputId": "c55d9ce7-8b5f-493d-c1d4-7d5f7a9f816f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making age bins \n",
    "data['age_bins']=data['age'].apply(lambda x:'18-30' if x<=30 else('31_40' if x>30 and x <=40 else('41_50' if x>40 and x<=50 else '50+')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "slOyxm60WLTU",
    "outputId": "b8ee709b-e8d8-4f19-ace6-a48c4951bd50"
   },
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data['age_bins'],drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "diWXgc8iWLTV",
    "outputId": "75d9eb13-e5e1-4ce1-f339-4f743d1bfe98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.concat([data,dummies],axis=1)\n",
    "data.drop(['age','height'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "EEC_UhuFWLTW",
    "outputId": "ed4b0008-363c-444d-949f-81d49e8a4b5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['education_level', 'drinks_often', 'drinks_rarely', 'drinks_socially',\n",
       "       'drugs_sometimes', 'orientation_gay', 'orientation_straight',\n",
       "       'banking / financial / real estate', 'education / academia',\n",
       "       'entertainment / media', 'executive / management', 'medicine / health',\n",
       "       'other_job', 'sales / marketing / biz dev',\n",
       "       'science / tech / engineering', 'student', 'likes dogs', 'likes cats',\n",
       "       'has dog', 'has cat', 'dislikes dogs', 'dislike cats',\n",
       "       'smokes_sometimes', 'smokes_yes', 'athletic', 'fat', 'fit', 'skinny',\n",
       "       'dropped_out_yes', 'location_same_city', 'location_same_state',\n",
       "       'single', 'astronomy', 'collectibles', 'cooking', 'dancing', 'food',\n",
       "       'game_video', 'gardening', 'movies', 'music', 'organising events',\n",
       "       'outdoor', 'photography', 'politics', 'read/write', 'singing',\n",
       "       'studying', 'travelling', 'benicia', 'berkeley', 'burlingame',\n",
       "       'daly city', 'el cerrito', 'emeryville', 'hayward', 'location_other',\n",
       "       'martinez', 'menlo park', 'oakland', 'pacifica', 'palo alto',\n",
       "       'redwood city', 'richmond', 'san francisco', 'san leandro', 'san mateo',\n",
       "       'san rafael', 'walnut creek', 'Explore', 'Full of life', 'Fun Partner',\n",
       "       'Knowing each other', 'Meeting new people', '31_40', '41_50', '50+'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.drop('age_bins',axis=1,inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "WQc86hT2WLTW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31_40',\n",
       " '41_50',\n",
       " '50+',\n",
       " 'Explore',\n",
       " 'Full of life',\n",
       " 'Fun Partner',\n",
       " 'Knowing each other',\n",
       " 'Meeting new people',\n",
       " 'astronomy',\n",
       " 'athletic',\n",
       " 'banking / financial / real estate',\n",
       " 'benicia',\n",
       " 'berkeley',\n",
       " 'burlingame',\n",
       " 'collectibles',\n",
       " 'cooking',\n",
       " 'daly city',\n",
       " 'dancing',\n",
       " 'dislike cats',\n",
       " 'dislikes dogs',\n",
       " 'drinks_often',\n",
       " 'drinks_rarely',\n",
       " 'drinks_socially',\n",
       " 'dropped_out_yes',\n",
       " 'drugs_sometimes',\n",
       " 'education / academia',\n",
       " 'education_level',\n",
       " 'el cerrito',\n",
       " 'emeryville',\n",
       " 'entertainment / media',\n",
       " 'executive / management',\n",
       " 'fat',\n",
       " 'fit',\n",
       " 'food',\n",
       " 'game_video',\n",
       " 'gardening',\n",
       " 'has cat',\n",
       " 'has dog',\n",
       " 'hayward',\n",
       " 'likes cats',\n",
       " 'likes dogs',\n",
       " 'location_other',\n",
       " 'location_same_city',\n",
       " 'location_same_state',\n",
       " 'martinez',\n",
       " 'medicine / health',\n",
       " 'menlo park',\n",
       " 'movies',\n",
       " 'music',\n",
       " 'oakland',\n",
       " 'organising events',\n",
       " 'orientation_gay',\n",
       " 'orientation_straight',\n",
       " 'other_job',\n",
       " 'outdoor',\n",
       " 'pacifica',\n",
       " 'palo alto',\n",
       " 'photography',\n",
       " 'politics',\n",
       " 'read/write',\n",
       " 'redwood city',\n",
       " 'richmond',\n",
       " 'sales / marketing / biz dev',\n",
       " 'san francisco',\n",
       " 'san leandro',\n",
       " 'san mateo',\n",
       " 'san rafael',\n",
       " 'science / tech / engineering',\n",
       " 'singing',\n",
       " 'single',\n",
       " 'skinny',\n",
       " 'smokes_sometimes',\n",
       " 'smokes_yes',\n",
       " 'student',\n",
       " 'studying',\n",
       " 'travelling',\n",
       " 'walnut creek']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_name = list(data)\n",
    "c_name.sort()\n",
    "c_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('logistic_regression_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# remove below test\n",
    "test=data.iloc[5,:]\n",
    "test=np.array(test).reshape((1,-1))\n",
    "test\n",
    "# Calculate the accuracy score and predict target values\n",
    "# score = pickle_model.score(Xtest, Ytest)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number=model.predict(test)\n",
    "cluster_number=int(cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing raw data with cluster labels for reccommendations\n",
    "# DB Values\n",
    "recc_data=pd.read_csv('Rawdata_withClusterLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       3\n",
       "4       3\n",
       "5       3\n",
       "20      3\n",
       "25      3\n",
       "       ..\n",
       "1980    3\n",
       "1990    3\n",
       "1996    3\n",
       "1997    3\n",
       "2000    3\n",
       "Name: cluster_labels, Length: 549, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recc_data=recc_data.loc[recc_data['cluster_labels']==cluster_number]\n",
    "recc_data['cluster_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age Gender Oriantation Conditions\n",
    "\n",
    "# Display Reccom"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Test_preprocessing_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
